# Awesome-Vision-Language-Models

## Papers
- [基础知识](#基础知识)
  - [模型构建框架](#模型构建框架)
- [Tasks](#tasks)
  - [检测](#检测)
  - [分割](#分割)
  - [VQA](#vqa)
  - [生成](#生成)
  - [视频](#视频)
- [Training](#training)
  - [数据集](#数据集)
  - [指令优化](#指令优化)
- [Application](#application)
  - [医疗](#医疗)
  - [文档大模型](#文档大模型)
  - [对话助手](#对话助手)
  - [自动驾驶](#自动驾驶)
  - [具身智能](#具身智能)
  - [AI Agent](#ai-agent)


## 基础知识
| Paper                                             |  Project WebSite | Code |
|---------------------------------------------------|:-------------:|:------------:|
|[**Vision-Language Models for Vision Tasks: A Survey**](https://arxiv.org/pdf/2304.00685.pdf)|-|[Code](https://github.com/jingyi0000/VLM_survey)|
|[**A Survey on Multimodal Large Language Models**](https://arxiv.org/pdf/2306.13549.pdf)|-|[Code](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)|
|[**Multimodal Foundation Models: From Specialists to General-Purpose Assistants**](https://arxiv.org/abs/2309.10020)|[Project Page](https://vlp-tutorial.github.io/2023/)|-|
### 模型构建框架

| Paper                                             |  Project WebSite | Code |
|---------------------------------------------------|:-------------:|:------------:|
|-|-|-|

## Tasks

### 检测
| Paper                                             |  Project WebSite | Code |
|---------------------------------------------------|:-------------:|:------------:|
|-|-|-|

### 分割
| Paper                                             |  Project WebSite | Code |
|---------------------------------------------------|:-------------:|:------------:|
|-|-|-|

### VQA
| Paper                                             |  Project WebSite | Code |
|---------------------------------------------------|:-------------:|:------------:|
|-|-|-|

### 生成
| Paper                                             |  Project WebSite | Code |
|---------------------------------------------------|:-------------:|:------------:|
|-|-|-|

### 视频
| Paper                                             |  Project WebSite | Code |
|---------------------------------------------------|:-------------:|:------------:|
|-|-|-|

## Training
### 数据集
| Paper                                             |  Project WebSite | Code |
|---------------------------------------------------|:-------------:|:------------:|
|-|-|-|

### 指令优化
| Paper                                             |  Project WebSite | Code |
|---------------------------------------------------|:-------------:|:------------:|
|-|-|-|

## Application
### 医疗
| Paper                                             |  Project WebSite | Code |
|---------------------------------------------------|:-------------:|:------------:|
|-|-|-|

### 文档大模型
| Paper                                             |  Project WebSite | Code |
|---------------------------------------------------|:-------------:|:------------:|
|-|-|-|

### 对话助手
| Paper                                             |  Project WebSite | Code |
|---------------------------------------------------|:-------------:|:------------:|
|-|-|-|

### 自动驾驶
| Paper | Project WebSite | Code |
| ----- | :-------------: | :--: |
| -     |        -        |  -   |
|       |                 |      |
|       |                 |      |
|       |                 |      |
|       |                 |      |
|       |                 |      |
|       |                 |      |
|       |                 |      |
|       |                 |      |

### 具身智能

#### Benchmark

| Paper                                             |  Project WebSite | Code |
|---------------------------------------------------|:-------------:|:------------:|
|[**iGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday Household Tasks**](https://arxiv.org/abs/2108.03272)|[Project Page](https://svl.stanford.edu/igibson/)|[Code](https://github.com/StanfordVL/iGibson)|
|[**BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments**](https://arxiv.org/abs/2108.03332)|[Project Page](https://behavior.stanford.edu/)|[Code](https://github.com/StanfordVL/behavior)|
|[**Habitat 2.0: Training Home Assistants to Rearrange their Habitat**]()|[Project Page](https://aihabitat.org/)|[Code](https://github.com/facebookresearch/habitat-lab/tree/v0.3.0)|
|[**Habitat 3.0: A Co-Habitat for Humans, Avatars and Robots**](https://arxiv.org/abs/2310.13724)|[Project Page](https://aihabitat.org/)|[Code](https://github.com/facebookresearch/habitat-lab/tree/v0.3.0)|
|[**Transporter Networks: Rearranging the Visual World for Robotic Manipulation**](https://arxiv.org/abs/2010.14406)|[Project Page](https://transporternets.github.io/)|[Code](https://github.com/google-research/ravens)|
|[**robosuite: A Modular Simulation Framework and Benchmark for Robot Learning**](https://arxiv.org/abs/2009.12293)|[Project Page](https://robosuite.ai/)|[Code](https://github.com/ARISE-Initiative/robosuite)|
|[**HandoverSim: A Simulation Framework and Benchmark for Human-to-Robot Object Handovers**](https://arxiv.org/abs/2205.09747)|[Project Page](https://handover-sim.github.io/)|[Code](https://github.com/NVlabs/handover-sim)|
|[**ManiSkill: Generalizable Manipulation Skill Benchmark with Large-Scale Demonstrations**](https://arxiv.org/abs/2107.14483)|[Project Page](https://sapien.ucsd.edu/challenges/maniskill/2021/)|[Code](https://github.com/haosulab/ManiSkill)|
|[**ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills**](https://arxiv.org/abs/2302.04659)|[Project Page](https://maniskill2.github.io/)|[Code](https://github.com/haosulab/ManiSkill2)|
|[**RLBench: The Robot Learning Benchmark & Learning Environment**](https://arxiv.org/abs/1909.12271)|[Project Page](https://sites.google.com/view/rlbench)|[Code](https://github.com/stepjam/RLBench)|
|[**ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks**](https://arxiv.org/abs/1912.01734)|[Project Page](https://askforalfred.com/)|[Code](https://github.com/askforalfred/alfred)|
|[**VIMA: General Robot Manipulation with Multimodal Prompts**](https://arxiv.org/abs/2210.03094)|[Project Page](https://vimalabs.github.io/)|[Code](https://github.com/vimalabs/VIMA)|
|[**VLMbench: A Compositional Benchmark for Vision-and-Language Manipulation**](https://arxiv.org/abs/2206.08522)|[Project Page](https://sites.google.com/ucsc.edu/vlmbench/home)|[Code](https://github.com/eric-ai-lab/VLMbench)|
|[**HandMeThat: Human-Robot Communication in Physical and Social Environments**](https://openreview.net/pdf?id=nUTemM6v9sv)|[Project Page](https://sites.google.com/view/hand-me-that/)|[Code](https://github.com/Simon-Wan/HandMeThat-Release)|
|[**RM-PRT: Realistic Robotic Manipulation Simulator and Benchmark with Progressive Reasoning Tasks**](https://arxiv.org/abs/2306.11335)|[Project Page](https://necolizer.github.io/RM-PRT/)|[Code](https://github.com/Necolizer/RM-PRT)|
|[**MO-VLN: A Multi-Task Benchmark for Open-set Zero-Shot Vision-and-Language Navigation**](https://arxiv.org/abs/2306.10322)|[Project Page](https://mligg23.github.io/MO-VLN-Site/)|[Code](https://github.com/mligg23/MO-VLN/)|
|[**Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation**](https://peract.github.io/paper/peract_corl2022.pdf)|[Project Page](https://peract.github.io/)|[Code](https://github.com/peract/peract)|
|[**CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning**](https://arxiv.org/pdf/2010.04296.pdf)|[Project Page](https://sites.google.com/view/causal-world/home)|[Code](https://github.com/rr-learning/CausalWorld#sim2real)|
|[**Interactive Language: Talking to Robots in Real Time**](https://arxiv.org/abs/2210.06407)|[Project Page](https://interactive-language.github.io/)|[Code](https://github.com/google-research/language-table)|

### AI Agent
| Paper                                             |  Project WebSite | Code |
|---------------------------------------------------|:-------------:|:------------:|
|-|-|-|
